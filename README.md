# oneLLM is All You Need!
High Performance Inference Serving Engine for popular LLMs based on GPU

# QuickStart
> cd oneLLM
> mkdir build && cd build
> cmake .. && make -j
