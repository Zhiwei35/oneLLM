# oneLLM is All You Need!
High Performance Inference Serving Engine for popular LLMs based on GPU

# QuickStart
## How to compile
1. cd oneLLM
2. mkdir build && cd build
3. cmake .. && make -j
